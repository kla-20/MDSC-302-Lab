{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regd No: 22231\n",
        "# Simple Neural Network\n"
      ],
      "metadata": {
        "id": "a4vPnjXAOJ1k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8KKWBtKC2aG"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn # importing torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision #installing torchvision package\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmuPxhthEEzR",
        "outputId": "4b1664b5-dfa4-4294-8e2a-270ba911ae88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # importing torch\n",
        "from torchvision import datasets # importing datasets from torchvision\n",
        "from torchvision.transforms import ToTensor, Lambda #importing ToTensor and Lambda\n",
        "\n",
        "ds_train = datasets.FashionMNIST(  # Getting dataset (training) from FashionMNIST\n",
        "    root = 'data',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor(), # this converts the image into a tensor\n",
        "    target_transform= Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0,torch.tensor(y), value=1))\n",
        "    # This creates a tensor of zeros with a shape of (10,), indicating that there are 10 possible classes or labels. The dtype is set to float.\n",
        ")\n",
        "\n",
        "test_dataset = datasets.FashionMNIST(  # Getting dataset (testing) from FashionMNIST\n",
        "    root = 'data',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = ToTensor(), # this converts the image into a tensor\n",
        "    target_transform= Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0,torch.tensor(y), value=1))\n",
        "    # This creates a tensor of zeros with a shape of (10,), indicating that there are 10 possible classes or labels. The dtype is set to float.\n",
        ")"
      ],
      "metadata": {
        "id": "wyiAc2DJEGbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa23ab5-ef8e-44e0-b346-9bb6173bcdd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 17051229.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 310182.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5474453.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 16271497.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this chunk is used to get the validation datset from the training dataset\n",
        "from torch.utils.data import random_split # importing random_split\n",
        "\n",
        "\n",
        "train_size = len(ds_train) # taking length of the training set\n",
        "val_size = int(0.1 * train_size)  # Taking 10% of the training set for validation\n",
        "\n",
        "train_dataset, val_dataset = random_split(ds_train, [train_size - val_size, val_size]) # seperating into 90% of training into new training and 10% of training to validation\n",
        "\n",
        "print(f\"Training set size: {len(train_dataset)}\")\n",
        "print(f\"Validation set size: {len(val_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb2dlSBtEqz3",
        "outputId": "59f44df3-6bd8-427e-b5ee-e45c91e91107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 54000\n",
            "Validation set size: 6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test set size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK5CkRXBKfSQ",
        "outputId": "9c398406-6298-4384-ff26-b0a68ae0321e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple neural network architecture\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.linear1 = nn.Linear(in_features=28*28, out_features=128)  # Input size 784 (28x28), output size 128\n",
        "        self.linear2 = nn.Linear(in_features=128, out_features=64)   # Input size 128, output size 64\n",
        "        self.linear3 = nn.Linear(in_features=64, out_features=10)    # Input size 64, output size 10 for 10 classes\n",
        "\n",
        "    def forward(self, x): # forward prop function\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        x = torch.relu(self.linear2(x))\n",
        "        x = torch.softmax(self.linear3(x),dim = 1)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the SimpleNet\n",
        "simple_net = SimpleNet()"
      ],
      "metadata": {
        "id": "OJe23rzRDcyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we will train it , validate it , test it"
      ],
      "metadata": {
        "id": "tjza-eK3c7IM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing the loss function and optimizer"
      ],
      "metadata": {
        "id": "pywtc-mddEIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim #importing optim"
      ],
      "metadata": {
        "id": "s1-HwUPAnwyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()  # Taking Cross Entropy Loss as the Loss function\n",
        "optimizer = optim.SGD(simple_net.parameters(), lr=0.01)  # optimizer is SGD stochastic gradient descent\n"
      ],
      "metadata": {
        "id": "AIbJ6ijQc-F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating DataLoaders\n",
        "Here Data Loaders are iteratable dataobject , where iterating over the batches and the data is fed into the model for processing."
      ],
      "metadata": {
        "id": "YM4ciP4mnl3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader # importing dataloader"
      ],
      "metadata": {
        "id": "seaZzhwsohQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading all the datasets into dataloader for feeding into the model by batches\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "28XuZ32moRpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Validating the Model"
      ],
      "metadata": {
        "id": "6noNeWxPozUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainval_loop(train_loader, val_loader, simple_net, criterion, optimizer): #parameters for the function\n",
        "    # Set the model to training mode before entering loops\n",
        "    simple_net.train()\n",
        "\n",
        "    # Training loop\n",
        "    size = len(train_loader.dataset)\n",
        "    for batch, (X, y) in enumerate(train_loader):\n",
        "        # Compute prediction and loss (training)\n",
        "        pred = simple_net(X)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad() # this is done to deny the involement of the previous gradients interference with the current ones\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            current = batch * len(X)\n",
        "            print(f\"Training loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\") #printing training loss\n",
        "\n",
        "    # Set the model to evaluation mode before validation\n",
        "    simple_net.eval()\n",
        "\n",
        "    # Validation loop\n",
        "    val_loss = 0\n",
        "    val_size = len(val_loader.dataset)\n",
        "    with torch.no_grad():  # No need to compute gradients during validation\n",
        "        for batch, (X, y) in enumerate(val_loader):\n",
        "            # Compute prediction and loss\n",
        "            pred = simple_net(X)\n",
        "            loss = criterion(pred, y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                current = batch * len(X)\n",
        "                print(f\"Validation loss: {loss.item():>7f}  [{current:>5d}/{val_size:>5d}]\") #printing validation loss\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Avg. Validation loss: {avg_val_loss:>7f}\") #printing average validation loss\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9G4e0yio8CXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the Model\n",
        "\n"
      ],
      "metadata": {
        "id": "iHqYvJ8b7rxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this chunk is important for testing the model\n",
        "def test_loop(test_loader, simple_net, criterion): # function test_loop taking these respective parameters\n",
        "    simple_net.eval() # setting the model to eval\n",
        "    size = len(test_loader.dataset)\n",
        "    num_batches = len(test_loader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            pred = simple_net(X)\n",
        "            test_loss += criterion(pred, y).item()\n",
        "\n",
        "            # Calculate the number of correct predictions in this batch\n",
        "            pred_labels = pred.argmax(dim=1)\n",
        "            y= y.argmax(dim=1)\n",
        "            #print(pred_labels,y)\n",
        "            correct += (pred_labels == y).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    accuracy = correct / size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*accuracy):.2f}%, Avg loss: {test_loss:.8f} \\n\") # printint the Accuracy (TEST) and the test_loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H1BKPND1-Ca5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Training and Validation"
      ],
      "metadata": {
        "id": "EYu8BXFO-deS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    trainval_loop(train_loader,val_loader, simple_net, criterion, optimizer) # running train_val loop by 10 times\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m48jPDxj-cjC",
        "outputId": "d0c52bdd-f466-4b31-9681-bf1b89c483bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Training loss: 2.303422  [    0/54000]\n",
            "Training loss: 2.304411  [ 6400/54000]\n",
            "Training loss: 2.302629  [12800/54000]\n",
            "Training loss: 2.298604  [19200/54000]\n",
            "Training loss: 2.298018  [25600/54000]\n",
            "Training loss: 2.296574  [32000/54000]\n",
            "Training loss: 2.297565  [38400/54000]\n",
            "Training loss: 2.295180  [44800/54000]\n",
            "Training loss: 2.294214  [51200/54000]\n",
            "Validation loss: 2.294480  [    0/ 6000]\n",
            "Avg. Validation loss: 2.293875\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Training loss: 2.291824  [    0/54000]\n",
            "Training loss: 2.294116  [ 6400/54000]\n",
            "Training loss: 2.289575  [12800/54000]\n",
            "Training loss: 2.287416  [19200/54000]\n",
            "Training loss: 2.283454  [25600/54000]\n",
            "Training loss: 2.285635  [32000/54000]\n",
            "Training loss: 2.280832  [38400/54000]\n",
            "Training loss: 2.265028  [44800/54000]\n",
            "Training loss: 2.270756  [51200/54000]\n",
            "Validation loss: 2.271298  [    0/ 6000]\n",
            "Avg. Validation loss: 2.268808\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Training loss: 2.273753  [    0/54000]\n",
            "Training loss: 2.251406  [ 6400/54000]\n",
            "Training loss: 2.252393  [12800/54000]\n",
            "Training loss: 2.253573  [19200/54000]\n",
            "Training loss: 2.239825  [25600/54000]\n",
            "Training loss: 2.217869  [32000/54000]\n",
            "Training loss: 2.205371  [38400/54000]\n",
            "Training loss: 2.190455  [44800/54000]\n",
            "Training loss: 2.133991  [51200/54000]\n",
            "Validation loss: 2.162807  [    0/ 6000]\n",
            "Avg. Validation loss: 2.163270\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Training loss: 2.145945  [    0/54000]\n",
            "Training loss: 2.106249  [ 6400/54000]\n",
            "Training loss: 2.145526  [12800/54000]\n",
            "Training loss: 2.103323  [19200/54000]\n",
            "Training loss: 2.090969  [25600/54000]\n",
            "Training loss: 2.084561  [32000/54000]\n",
            "Training loss: 2.001060  [38400/54000]\n",
            "Training loss: 2.064585  [44800/54000]\n",
            "Training loss: 1.965770  [51200/54000]\n",
            "Validation loss: 2.006072  [    0/ 6000]\n",
            "Avg. Validation loss: 2.018586\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Training loss: 2.069394  [    0/54000]\n",
            "Training loss: 1.916804  [ 6400/54000]\n",
            "Training loss: 2.043878  [12800/54000]\n",
            "Training loss: 1.996500  [19200/54000]\n",
            "Training loss: 1.931827  [25600/54000]\n",
            "Training loss: 1.970034  [32000/54000]\n",
            "Training loss: 1.930200  [38400/54000]\n",
            "Training loss: 2.013499  [44800/54000]\n",
            "Training loss: 1.945086  [51200/54000]\n",
            "Validation loss: 1.928170  [    0/ 6000]\n",
            "Avg. Validation loss: 1.936177\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Training loss: 1.976092  [    0/54000]\n",
            "Training loss: 1.945639  [ 6400/54000]\n",
            "Training loss: 1.914993  [12800/54000]\n",
            "Training loss: 1.867094  [19200/54000]\n",
            "Training loss: 1.992125  [25600/54000]\n",
            "Training loss: 1.873842  [32000/54000]\n",
            "Training loss: 1.870334  [38400/54000]\n",
            "Training loss: 2.009403  [44800/54000]\n",
            "Training loss: 1.854114  [51200/54000]\n",
            "Validation loss: 1.888839  [    0/ 6000]\n",
            "Avg. Validation loss: 1.894895\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Training loss: 1.974012  [    0/54000]\n",
            "Training loss: 1.920589  [ 6400/54000]\n",
            "Training loss: 1.814460  [12800/54000]\n",
            "Training loss: 1.827810  [19200/54000]\n",
            "Training loss: 1.860390  [25600/54000]\n",
            "Training loss: 1.732262  [32000/54000]\n",
            "Training loss: 1.830327  [38400/54000]\n",
            "Training loss: 1.850322  [44800/54000]\n",
            "Training loss: 1.832794  [51200/54000]\n",
            "Validation loss: 1.781287  [    0/ 6000]\n",
            "Avg. Validation loss: 1.811429\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Training loss: 1.847601  [    0/54000]\n",
            "Training loss: 1.799236  [ 6400/54000]\n",
            "Training loss: 1.783507  [12800/54000]\n",
            "Training loss: 1.876009  [19200/54000]\n",
            "Training loss: 1.724653  [25600/54000]\n",
            "Training loss: 1.716430  [32000/54000]\n",
            "Training loss: 1.846144  [38400/54000]\n",
            "Training loss: 1.792570  [44800/54000]\n",
            "Training loss: 1.746024  [51200/54000]\n",
            "Validation loss: 1.751426  [    0/ 6000]\n",
            "Avg. Validation loss: 1.786645\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Training loss: 1.726854  [    0/54000]\n",
            "Training loss: 1.731249  [ 6400/54000]\n",
            "Training loss: 1.866798  [12800/54000]\n",
            "Training loss: 1.696626  [19200/54000]\n",
            "Training loss: 1.782051  [25600/54000]\n",
            "Training loss: 1.756971  [32000/54000]\n",
            "Training loss: 1.832733  [38400/54000]\n",
            "Training loss: 1.749529  [44800/54000]\n",
            "Training loss: 1.745496  [51200/54000]\n",
            "Validation loss: 1.735967  [    0/ 6000]\n",
            "Avg. Validation loss: 1.768203\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Training loss: 1.814849  [    0/54000]\n",
            "Training loss: 1.688771  [ 6400/54000]\n",
            "Training loss: 1.852808  [12800/54000]\n",
            "Training loss: 1.712372  [19200/54000]\n",
            "Training loss: 1.775273  [25600/54000]\n",
            "Training loss: 1.781756  [32000/54000]\n",
            "Training loss: 1.752838  [38400/54000]\n",
            "Training loss: 1.770545  [44800/54000]\n",
            "Training loss: 1.759383  [51200/54000]\n",
            "Validation loss: 1.727533  [    0/ 6000]\n",
            "Avg. Validation loss: 1.750386\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "p2g3u39P_iZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loop(test_loader, simple_net, criterion) #test_loop running and checking the results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9Ggot7m_gBR",
        "outputId": "31f0e3f2-ee95-4b02-c730-2e4ec720468d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 73.61%, Avg loss: 1.75831701 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}