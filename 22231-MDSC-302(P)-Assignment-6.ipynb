{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d32abd5",
   "metadata": {},
   "source": [
    "##### Name: K Lalith Aditya\n",
    "##### Regd No: 22231\n",
    "##### OPTIMIZING MODEL PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74645e37",
   "metadata": {},
   "source": [
    "\n",
    "After having the model and data its time to train , validate and test our model by optimizing its parameters on our data. Training a model is an iterative process\n",
    "\n",
    "In each iteration the model makes a guess about the output , caluculates the error i.e. the loss function. collects the derivatives of the error with respect to the parameters, and optimizes the parameters using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9094d5",
   "metadata": {},
   "source": [
    "##### Pre Requisite Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18a95110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Import the PyTorch library\n",
    "from torch import nn\n",
    "# Import the `nn` module from PyTorch, which contains the neural network classes\n",
    "from torch.utils.data import DataLoader\n",
    "# Import the `DataLoader` class from PyTorch, which is used to load data in batches\n",
    "from torchvision import datasets\n",
    "# Import the `datasets` module from TorchVision, which contains the FashionMNIST dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "# Import the `ToTensor` transform from TorchVision, which converts images to tensors\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "# Load the FashionMNIST dataset, specifying that we want the training data and to download it if it doesn't exist already. We also apply the `ToTensor` transform to convert the images to tensors.\n",
    "\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "# Load the FashionMNIST dataset, specifying that we want the test data and to download it if it doesn't exist already. We also apply the `ToTensor` transform to convert the images to tensors.\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size = 64)\n",
    "# Create a `DataLoader` object for the training data, specifying that we want to load the data in batches of size 64.\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size = 64)\n",
    "# Create a `DataLoader` object for the test data, specifying that we want to load the data in batches of size 64.\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    # Define a class called `NeuralNetwork` that inherits from the `nn.Module` class.\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Call the `__init__()` method of the `nn.Module` class.\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Create a `nn.Flatten` object to flatten the input images.\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28,512),# linear layer with 28*28 inputs and 512 outputs\n",
    "            nn.ReLU(), # ReLU activation\n",
    "            nn.Linear(512,512), # linear layer with 512 inputs and 512 outputs\n",
    "            nn.ReLU(), # ReLU activation\n",
    "            nn.Linear(512,18) # linear layer 512 inputs and 18 outputs\n",
    "        )\n",
    "        \n",
    "    def forward(self, x): # function forward\n",
    "            x = self.flatten(x) # flattens the inputs\n",
    "            logits = self.linear_relu_stack(x) # inputs into relu stack\n",
    "            return logits\n",
    "        \n",
    "model = NeuralNetwork()        # calling the instance of NeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd964a3d",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca038e",
   "metadata": {},
   "source": [
    "Hyper parameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact the model training and convergence rates\n",
    "\n",
    "Hyperparameters for training:\n",
    "\n",
    "Number of Epochs: The number times to iterate over the dataset\n",
    "\n",
    "Batch Size: The number of data samples propagated through the network before the parameters are updated\n",
    "\n",
    "Learning Rate: How much to update model parameters at each batch/epoch. Smaller values yield slow learning speed, while large values result in unpredictable behaviour during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a923eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d722820c",
   "metadata": {},
   "source": [
    "#### Optimization Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c2345",
   "metadata": {},
   "source": [
    "Once the hyper parameters are set , we can train and optimize our model with an optimization loop. Each iteration of the optimization loop is called an epoch.\n",
    "\n",
    "Each Epoch consists of two main parts:\n",
    "1 The Train Loop: Iterate over the training set and try to converge to optimal parameters.\n",
    "2 The Validation/Test Loop: Iterate over the test dataset to check if the model performance is improving.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea7f606",
   "metadata": {},
   "source": [
    "#### Loss Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93611e0d",
   "metadata": {},
   "source": [
    "Loss function measures the dissimilarity of obtained result to the target value, and its the loss function to be minimized during the training.\n",
    "To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.\n",
    "\n",
    "Common Loss Functions:\n",
    "1. nn.MSELoss (Mean Square Error) for regression\n",
    "2. nn.NNLLoss (Negative Log Likelihood) for classification\n",
    "3. nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbdbd7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the cross entropy loss function\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f045f",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4370cab5",
   "metadata": {},
   "source": [
    "Optimization is the process of adjusting the model parameters to reduce the model error in training step. Optimization algorithms define how this process is performed.\n",
    "In optimizer object the optimization logic is encapsulated.\n",
    "Many other optimizers are:-\n",
    "1. ADAM\n",
    "2. RMSProp\n",
    "\n",
    "Initializing the optimizer by registering the model's parameters that need to be trained, and passing the learning rate hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42c096c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac0e9c7",
   "metadata": {},
   "source": [
    "Inside the training loop. optimization happens in three steps:\n",
    "\n",
    "1. Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "\n",
    "2. Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t each parameter.\n",
    "\n",
    "Once we have our gradients, we call optimizer.step() to adjust parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4f8b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset) # Get the total size of the dataset\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train() # Set the model to training mode (important for batch normalization and dropout layers)\n",
    "    for batch, (X, y) in enumerate(dataloader): # Iterate over the data batches\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X) # Make predictions using the model\n",
    "        loss = loss_fn(pred, y) # Calculate the loss between predictions and true labels\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward() # Compute gradients for model parameters using backpropagation\n",
    "        optimizer.step() # Update model parameters using the specified optimizer\n",
    "        optimizer.zero_grad() # Reset gradients to zero for the next iteration\n",
    "\n",
    "        if batch % 100 == 0: # Print loss information every 100 batches\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval() # Set the model to evaluation mode (important for batch normalization and dropout layers)\n",
    "    size = len(dataloader.dataset) # Get the total size of the dataset\n",
    "    num_batches = len(dataloader) # Get the number of batches in the dataloader\n",
    "    test_loss, correct = 0, 0 # Initialize variables to track test loss and correct predictions\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader: # Iterate over the data batches\n",
    "            pred = model(X) # Make predictions using the model\n",
    "            test_loss += loss_fn(pred, y).item() # Calculate the loss on test data\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # Count correct predictions\n",
    "\n",
    "    test_loss /= num_batches # Calculate the average test loss\n",
    "    correct /= size # Calculate the accuracy by dividing the correct predictions by the total dataset size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1a95f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.797336  [   64/60000]\n",
      "loss: 0.861056  [ 6464/60000]\n",
      "loss: 0.639708  [12864/60000]\n",
      "loss: 0.836981  [19264/60000]\n",
      "loss: 0.736338  [25664/60000]\n",
      "loss: 0.739897  [32064/60000]\n",
      "loss: 0.819592  [38464/60000]\n",
      "loss: 0.806062  [44864/60000]\n",
      "loss: 0.796040  [51264/60000]\n",
      "loss: 0.768770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.759868 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.760314  [   64/60000]\n",
      "loss: 0.832050  [ 6464/60000]\n",
      "loss: 0.609199  [12864/60000]\n",
      "loss: 0.813219  [19264/60000]\n",
      "loss: 0.714624  [25664/60000]\n",
      "loss: 0.715436  [32064/60000]\n",
      "loss: 0.795119  [38464/60000]\n",
      "loss: 0.790725  [44864/60000]\n",
      "loss: 0.774587  [51264/60000]\n",
      "loss: 0.748458  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.738050 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.727878  [   64/60000]\n",
      "loss: 0.805841  [ 6464/60000]\n",
      "loss: 0.582827  [12864/60000]\n",
      "loss: 0.793039  [19264/60000]\n",
      "loss: 0.695862  [25664/60000]\n",
      "loss: 0.695012  [32064/60000]\n",
      "loss: 0.772319  [38464/60000]\n",
      "loss: 0.776668  [44864/60000]\n",
      "loss: 0.755914  [51264/60000]\n",
      "loss: 0.729906  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.718482 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.698784  [   64/60000]\n",
      "loss: 0.782089  [ 6464/60000]\n",
      "loss: 0.559679  [12864/60000]\n",
      "loss: 0.775294  [19264/60000]\n",
      "loss: 0.679436  [25664/60000]\n",
      "loss: 0.677849  [32064/60000]\n",
      "loss: 0.750973  [38464/60000]\n",
      "loss: 0.763848  [44864/60000]\n",
      "loss: 0.739562  [51264/60000]\n",
      "loss: 0.713002  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.700745 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.673040  [   64/60000]\n",
      "loss: 0.760135  [ 6464/60000]\n",
      "loss: 0.539371  [12864/60000]\n",
      "loss: 0.759633  [19264/60000]\n",
      "loss: 0.664953  [25664/60000]\n",
      "loss: 0.662876  [32064/60000]\n",
      "loss: 0.730812  [38464/60000]\n",
      "loss: 0.751944  [44864/60000]\n",
      "loss: 0.724966  [51264/60000]\n",
      "loss: 0.697372  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.684418 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.649762  [   64/60000]\n",
      "loss: 0.739701  [ 6464/60000]\n",
      "loss: 0.521179  [12864/60000]\n",
      "loss: 0.745479  [19264/60000]\n",
      "loss: 0.652054  [25664/60000]\n",
      "loss: 0.649886  [32064/60000]\n",
      "loss: 0.711721  [38464/60000]\n",
      "loss: 0.740867  [44864/60000]\n",
      "loss: 0.711926  [51264/60000]\n",
      "loss: 0.682818  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.669298 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.628700  [   64/60000]\n",
      "loss: 0.720685  [ 6464/60000]\n",
      "loss: 0.504806  [12864/60000]\n",
      "loss: 0.732472  [19264/60000]\n",
      "loss: 0.640622  [25664/60000]\n",
      "loss: 0.638429  [32064/60000]\n",
      "loss: 0.693762  [38464/60000]\n",
      "loss: 0.730683  [44864/60000]\n",
      "loss: 0.700431  [51264/60000]\n",
      "loss: 0.669008  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.655310 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.609730  [   64/60000]\n",
      "loss: 0.703114  [ 6464/60000]\n",
      "loss: 0.489912  [12864/60000]\n",
      "loss: 0.720446  [19264/60000]\n",
      "loss: 0.630333  [25664/60000]\n",
      "loss: 0.628180  [32064/60000]\n",
      "loss: 0.676948  [38464/60000]\n",
      "loss: 0.721432  [44864/60000]\n",
      "loss: 0.690396  [51264/60000]\n",
      "loss: 0.655996  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.642358 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.592512  [   64/60000]\n",
      "loss: 0.686888  [ 6464/60000]\n",
      "loss: 0.476417  [12864/60000]\n",
      "loss: 0.709280  [19264/60000]\n",
      "loss: 0.621251  [25664/60000]\n",
      "loss: 0.619100  [32064/60000]\n",
      "loss: 0.661168  [38464/60000]\n",
      "loss: 0.713243  [44864/60000]\n",
      "loss: 0.681826  [51264/60000]\n",
      "loss: 0.643653  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.630425 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.576844  [   64/60000]\n",
      "loss: 0.671803  [ 6464/60000]\n",
      "loss: 0.464167  [12864/60000]\n",
      "loss: 0.698938  [19264/60000]\n",
      "loss: 0.613148  [25664/60000]\n",
      "loss: 0.611059  [32064/60000]\n",
      "loss: 0.646414  [38464/60000]\n",
      "loss: 0.705996  [44864/60000]\n",
      "loss: 0.674460  [51264/60000]\n",
      "loss: 0.631863  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.619437 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() # cross entropy loss is the loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Stochastic Gradient descent is the optimizer\n",
    "\n",
    "epochs = 10 # 10 epochs\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer) # Train the model for one epoch using the train_loop function\n",
    "    test_loop(test_dataloader, model, loss_fn) #Evaluate the model on the test dataset using the test_loop function\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
