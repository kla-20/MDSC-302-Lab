{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aadca9",
   "metadata": {},
   "source": [
    "##### Name: K Lalith Aditya\n",
    "##### Regd No: 22231\n",
    "##### AUTOMATIC DIFFERENTIATION WITH TORCH.AUTOGRAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6454c4f5",
   "metadata": {},
   "source": [
    "<h4>When training neural networks, the most frequently used algorithm is back propagation. In this algorithm, parameters (model weights) are adjusted according to the gradient of the loss function with respect to the given parameter.\n",
    "\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine called torch.autograd. It supports automatic computation of gradient for any computational graph.</h4>\n",
    "\n",
    "Considering the simplest one-layer neural network, with input x, parameters w and b, and some loss function. It can be defined like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324c5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5) # input tensor\n",
    "y = torch.zeros(3) # executed output\n",
    "w = torch.randn(5,3, requires_grad=True)\n",
    "# creates a tensor w of size 5x3 is created with random values from a normal distribution. The requires_grad=True flag indicates that gradients with respect to this tensor should be computed during backpropagation.\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "# Similarly, a tensor b of size 3 is created with random values, and the requires_grad=True flag enables gradient computation for this tensor.\n",
    "z = torch.matmul(x, w)+b # does matrix multiplication of x, w and adds tensor b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)\n",
    "#It compares the values in z (predicted logits) with y (expected labels) and calculates the loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "791b62ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000001E86B0E19F0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000001E86B0E3A90>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "# This line prints the gradient function associated with the tensor z. The grad_fn attribute of a tensor stores the reference to the function that created it. By printing z.grad_fn, you can see the operation (such as matrix multiplication and addition) that generated z.\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")\n",
    "# Similarly, this line prints the gradient function associated with the tensor loss. Since loss is computed using the binary cross-entropy loss function, the gradient function will correspond to this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7fcfe7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters, namely, we need \\nunder some fixed values of x and y. To compute those derivatives, we call loss.backward(), and then retrieve the values from w.grad and b.grad:\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters, namely, we need \n",
    "under some fixed values of x and y. To compute those derivatives, we call loss.backward(), and then retrieve the values from w.grad and b.grad:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "079f203c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1833, 0.2676, 0.2318],\n",
      "        [0.1833, 0.2676, 0.2318],\n",
      "        [0.1833, 0.2676, 0.2318],\n",
      "        [0.1833, 0.2676, 0.2318],\n",
      "        [0.1833, 0.2676, 0.2318]])\n",
      "tensor([0.1833, 0.2676, 0.2318])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72f290b",
   "metadata": {},
   "source": [
    "#### Disabling Gradient Tracking\n",
    "By default, all tensors with requires_grad=True are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. We can stop tracking computations by surrounding our computation code with torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de65e46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b # operation on tensor\n",
    "print(z.requires_grad) # wether tensor requires grad\n",
    "\n",
    "with torch.no_grad(): # torch.no_grad() context disables gradient tracking. \n",
    "    z = torch.matmul(x, w)+b \n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "949678b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Another aproach is to use detach() method\n",
    "\n",
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach() # disabling the gradient tracking\n",
    "print(z_det.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
